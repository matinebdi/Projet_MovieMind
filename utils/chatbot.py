from langchain_groq import ChatGroq
# Import des biblioth√®ques n√©cessaires
from langchain_groq import ChatGroq
from langchain_openai import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from dotenv import load_dotenv
import streamlit as st
import os
from utils.vectorestore import load_vectorstore  # Simplification de l'import

# Chargement des variables d'environnement (cl√©s API, etc.)
load_dotenv()

# Initialisation du mod√®le d'embedding
@st.cache_resource  # Cache la ressource pour √©viter de la recharger √† chaque requ√™te
def get_embeddings():
    """
    Initialise le mod√®le d'embedding d'OpenAI.
    
    Les embeddings sont des repr√©sentations vectorielles du texte qui permettent
    de mesurer la similarit√© s√©mantique entre diff√©rents textes.
    
    Returns:
        OpenAIEmbeddings: Instance du mod√®le d'embedding configur√©
    """
    openai_api_key = st.secrets["OPENAI_API_KEY"]
    
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
    return OpenAIEmbeddings(
        api_key=openai_api_key,
        model="text-embedding-3-small"  # Mod√®le plus l√©ger et √©conomique
    )

# Initialisation du mod√®le de langage (LLM)
@st.cache_resource
def get_llm():
    """
    Initialise le mod√®le de langage Grok.
    
    Grok est un mod√®le de langage d√©velopp√© par xAI,
    offrant des performances de haut niveau pour la g√©n√©ration de texte.
    
    Returns:
        ChatOpenAI: Instance du mod√®le de langage configur√©
    """
    gorq_api_key = st.secrets["GROQ_API_KEY"]
    if not groq_api_key:
        raise ValueError("GROQ_API_KEY non trouv√©e dans les variables d'environnement")
    
    return ChatOpenAI(
        api_key=groq_api_key,
        temperature=0.7,  # Contr√¥le la cr√©ativit√© des r√©ponses (0=conservateur, 1=cr√©atif)
         model="llama-3.3-70b-versatile",
        
    )

# Chargement de la base de donn√©es vectorielle
@st.cache_resource
def get_vectorstore():
    """
    Charge la base de donn√©es vectorielle FAISS
    """
    return load_vectorstore()

# Cr√©ation de la cha√Æne de conversation
def get_conversation_chain(vectorstore):
    """
    Configure la cha√Æne de conversation qui combine recherche et dialogue.
    
    Cette fonction:
    1. Initialise la m√©moire pour garder le contexte de la conversation
    2. Cr√©e un template de prompt qui guide le comportement de l'assistant
    3. Configure la cha√Æne de conversation qui utilise le LLM et la recherche
    
    Args:
        vectorstore: Base de donn√©es vectorielle pour la recherche
        
    Returns:
        ConversationalRetrievalChain: Cha√Æne de conversation configur√©e
    """
    # Initialisation de la m√©moire pour le contexte
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    # Cr√©ation du template de prompt
    template = """Tu es un assistant de recommandation de films amical,
                 comp√©tent et cr√©atif. Ta mission est de fournir des recommandations de films d√©taill√©es,
                 engageantes et personnalis√©es, en t'appuyant sur les √©l√©ments de contexte suivants.
                 Pour chaque recommandation, utilise le format markdown ci-dessous :

---s

### üé¨ [Nom du film]  
**‚è±Ô∏è Dur√©e :** [dur√©e]  
**üìä Genre :** [genre]  
**üåü Note IMDb :** [note]  

#### üé• Synopsis  
[description d√©taill√©e du film]  

#### üë• Casting principal  
- [Acteur 1]  
- [Acteur 2]  
...  

#### üìù Pourquoi le regarder ?  
[explication de l‚Äôint√©r√™t ou des points forts du film]  

---

<contexte>  
{context}  
</contexte>  

Historique de conversation :  
{chat_history}  

**Instructions :**  
- Si la question pos√©e est li√©e aux recommandations de films, r√©ponds avec une ou plusieurs suggestions en suivant le format indiqu√©.  
- Si la question est hors sujet, r√©ponds de mani√®re polie et concise : "Malheureusement, je ne peux pas vous r√©pondre √† ce sujet."  

Humain : {question}  
Assistant : [Ta r√©ponse ici]"""

    
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "chat_history", "question"]
    )
    
    # Configuration de la cha√Æne de conversation
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=get_llm(),
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),  # R√©cup√®re les 3 documents les plus pertinents
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt}
    )
    return conversation_chain





    



